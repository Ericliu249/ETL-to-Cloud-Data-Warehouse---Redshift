# Data Warehouse in Redshift

## Backgroud

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Objective

Build an ETL pipeline that extracts their data from S3, stage them in Redshift, and transform data into a set of dimensional tables for the analytics team to continue finding insights in what songs the users are listening to. 

## Dependencies

- Python 3.5+
- psycopg2
- configparser

## Data Sources
Datasets reside in S3:

Song data: `s3://udacity-dend/song_data`  
Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

`song_data/A/B/C/TRABCEI128F424C983.json`
`song_data/A/A/B/TRAABJL12903CDCF1A.json`
`

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

 ```log_data/2018/11/2018-11-12-events.json```
 ```log_data/2018/11/2018-11-13-events.json```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![example](example.png)

## Schema for Song Play Analysis
Using the song and log datasets, we created a star schema optimized for queries on song play analysis. This includes the following tables.

### Fact Table
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
     - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*  
     
     As this table records user transactions that probably to become very big in the log run, so we sort, split then distribute the partitions to different nodes based on songplay_id.

### Dimension Tables
2. **users** - users in the app
     - *user_id, first_name, last_name, gender, level*
     
     
3. **songs** - songs in music database
     - *song_id, title, artist_id, year, duration*


4. **artists** - artists in music database
     - *artist_id, name, location, latitude, longitude*
     
     
5. **time** - timestamps of records in **songplays** broken down into specific units
     - *start_time, hour, day, week, month, year, weekday*
     
     
These dimension tables are smaller compared to the songplays fact able, and are frequently refered by analysis team, so we use the one of the partititon column as key to distribute the records over nodes in cluster.
     
## Guideline
1. ```create_tables.py```: create the fact and dimension tables for the star schema in Redshift.
2. ```etl.py```: load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift.
3. ```sql_queries.py``` : define you SQL statements, which will be imported into the two other files above.
3. ```dwh.cfg```: stores configuration information about AWS Redshift cluster, IAM role used for the cluster, and the S3 data source

## Project Steps

### Create Tables
1. Launch a redshift cluster and create an IAM role that has read access to S3.
2. Add redshift database and IAM role info to ```dwh.cfg```.
3. Run ```create_tables.py``` to create the fact and dimension tables.
4. Check the table schemas in your redshift database. 

### Build ETL Pipeline
5. Run **etl.py** to load data from S3 to staging tables on Redshift as well as to load data from staging tables to analytics tables on Redshift.
6. Run ant  analytic queries you are interested on your Redshift database to compare your results with the expected results.

Example queries: How many songs are played in Oakland Hayward, Sanfranciso?
```sql
SELECT COUNT(*) FROM  public.songplays
WHERE location = 'San Francisco-Oakland-Hayward, CA';
```
Results: `3`

Remember to run ```create_tables.py``` before running ```etl.py``` to reset the tables. Delete your redshift cluster when finished to avoid unexpected cost.